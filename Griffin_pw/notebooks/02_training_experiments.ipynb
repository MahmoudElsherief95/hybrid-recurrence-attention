{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7283e95f",
   "metadata": {},
   "source": [
    "# Griffin Model Training: Comparative Training Experiments\n",
    "\n",
    "This notebook demonstrates how to train and compare the three model architectures:\n",
    "- **Griffin**: Hybrid recurrence + attention model\n",
    "- **Hawk**: Pure recurrent model\n",
    "- **Local Attention**: Pure attention model\n",
    "\n",
    "We'll train these models on both MQAR and Chomsky hierarchy datasets and track their learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dc855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('.').absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules\n",
    "from models.griffin.griffin_model import GriffinModel\n",
    "from models.hawk.hawk_model import HawkModel\n",
    "from models.local_attention.attention_model import LocalAttentionModel\n",
    "from datasets.mqar import MQARDataset\n",
    "from datasets.chomsky import ChomskyDataset\n",
    "from training.trainer import Trainer\n",
    "from evaluation.evaluator import ModelEvaluator\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9fe1d",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "Let's load the model configurations and set up training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ede57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "config_dir = project_root / 'config'\n",
    "\n",
    "def load_config(config_name):\n",
    "    config_path = config_dir / f'{config_name}.yaml'\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load model configs\n",
    "griffin_config = load_config('griffin_config')\n",
    "hawk_config = load_config('hawk_config')\n",
    "attention_config = load_config('attention_config')\n",
    "training_config = load_config('training_config')\n",
    "\n",
    "print(\"Configurations loaded successfully!\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Epochs: {training_config['num_epochs']}\")\n",
    "print(f\"  Batch size: {training_config['batch_size']}\")\n",
    "print(f\"  Learning rate: {training_config['learning_rate']}\")\n",
    "print(f\"  Weight decay: {training_config['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4788b4",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "Create training datasets for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac69d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets with notebook-friendly sizes\n",
    "print(\"Creating datasets...\")\n",
    "\n",
    "# MQAR dataset\n",
    "train_mqar, val_mqar, test_mqar = create_mqar_datasets(\n",
    "    train_size=2000,  # Reduced for notebook\n",
    "    val_size=400,\n",
    "    test_size=400,\n",
    "    seq_len=griffin_config['max_seq_len'],\n",
    "    vocab_size=griffin_config['vocab_size'],\n",
    "    num_kv_pairs=8,\n",
    "    num_queries=2\n",
    ")\n",
    "\n",
    "# Parentheses dataset\n",
    "train_paren, val_paren, test_paren = create_chomsky_datasets(\n",
    "    dataset_type=\"parentheses\",\n",
    "    train_size=2000,\n",
    "    val_size=400,\n",
    "    test_size=400,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = training_config['batch_size']\n",
    "\n",
    "datasets = {\n",
    "    'MQAR': {\n",
    "        'train': train_mqar.create_dataloader(batch_size=batch_size, shuffle=True),\n",
    "        'val': val_mqar.create_dataloader(batch_size=batch_size, shuffle=False),\n",
    "        'test': test_mqar.create_dataloader(batch_size=batch_size, shuffle=False),\n",
    "        'vocab_size': train_mqar.get_vocab_size()\n",
    "    },\n",
    "    'Parentheses': {\n",
    "        'train': train_paren.create_dataloader(batch_size=batch_size, shuffle=True),\n",
    "        'val': val_paren.create_dataloader(batch_size=batch_size, shuffle=False),\n",
    "        'test': test_paren.create_dataloader(batch_size=batch_size, shuffle=False),\n",
    "        'vocab_size': train_paren.total_vocab_size\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nDatasets created:\")\n",
    "for name, data in datasets.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Train batches: {len(data['train'])}\")\n",
    "    print(f\"    Val batches: {len(data['val'])}\")\n",
    "    print(f\"    Vocab size: {data['vocab_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eeb476",
   "metadata": {},
   "source": [
    "## 3. Model Initialization\n",
    "\n",
    "Create and initialize all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d94707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(vocab_size):\n",
    "    \"\"\"Create all three models with the given vocabulary size.\"\"\"\n",
    "    \n",
    "    # Griffin model\n",
    "    griffin = GriffinModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=griffin_config['d_model'],\n",
    "        num_layers=griffin_config['num_layers'],\n",
    "        num_heads=griffin_config['num_heads'],\n",
    "        max_seq_len=griffin_config['max_seq_len'],\n",
    "        local_window=griffin_config['local_window'],\n",
    "        mixing_alpha=griffin_config['mixing_alpha']\n",
    "    )\n",
    "    \n",
    "    # Hawk model\n",
    "    hawk = HawkModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=hawk_config['d_model'],\n",
    "        num_layers=hawk_config['num_layers'],\n",
    "        max_seq_len=hawk_config['max_seq_len']\n",
    "    )\n",
    "    \n",
    "    # Local Attention model\n",
    "    local_attention = LocalAttentionModel(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=attention_config['d_model'],\n",
    "        num_layers=attention_config['num_layers'],\n",
    "        num_heads=attention_config['num_heads'],\n",
    "        max_seq_len=attention_config['max_seq_len'],\n",
    "        local_window=attention_config['local_window']\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'Griffin': griffin,\n",
    "        'Hawk': hawk,\n",
    "        'Local Attention': local_attention\n",
    "    }\n",
    "\n",
    "# Models will be created per dataset due to different vocab sizes\n",
    "print(\"Model creation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31746a67",
   "metadata": {},
   "source": [
    "## 4. Training Function\n",
    "\n",
    "Define a helper function to train models and track metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_experiment(model, model_name, train_loader, val_loader, \n",
    "                          dataset_name, num_epochs=5):\n",
    "    \"\"\"Train a single model and return training history.\"\"\"\n",
    "    \n",
    "    print(f\"Training {model_name} on {dataset_name}...\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = BaseTrainer(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        learning_rate=training_config['learning_rate'],\n",
    "        weight_decay=training_config['weight_decay'],\n",
    "        warmup_steps=100,  # Reduced for notebook\n",
    "        save_dir=project_root / 'notebooks' / 'training_outputs',\n",
    "        experiment_name=f\"{model_name}_{dataset_name}\",\n",
    "        mixed_precision=training_config.get('mixed_precision', False)\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_perplexity': [],\n",
    "        'val_perplexity': [],\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"  Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_metrics = trainer.train_epoch(train_loader)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = trainer.validate(val_loader)\n",
    "        \n",
    "        # Store metrics\n",
    "        history['train_loss'].append(train_metrics['loss'])\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['train_perplexity'].append(train_metrics['perplexity'])\n",
    "        history['val_perplexity'].append(val_metrics['perplexity'])\n",
    "        history['epochs'].append(epoch + 1)\n",
    "        \n",
    "        print(f\"    Train Loss: {train_metrics['loss']:.4f}, \"\n",
    "              f\"Val Loss: {val_metrics['loss']:.4f}, \"\n",
    "              f\"Val PPL: {val_metrics['perplexity']:.4f}\")\n",
    "    \n",
    "    return history, trainer\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4a655",
   "metadata": {},
   "source": [
    "## 5. Training Experiments\n",
    "\n",
    "Now let's train all models on both datasets and compare their learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training experiments\n",
    "all_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Reduced epochs for notebook demonstration\n",
    "num_epochs = 3\n",
    "\n",
    "for dataset_name, dataset_info in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING ON {dataset_name} DATASET\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create models for this dataset\n",
    "    models = create_models(dataset_info['vocab_size'])\n",
    "    \n",
    "    dataset_results = {}\n",
    "    dataset_models = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            # Train the model\n",
    "            history, trainer = train_model_experiment(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                train_loader=dataset_info['train'],\n",
    "                val_loader=dataset_info['val'],\n",
    "                dataset_name=dataset_name,\n",
    "                num_epochs=num_epochs\n",
    "            )\n",
    "            \n",
    "            dataset_results[model_name] = history\n",
    "            dataset_models[model_name] = trainer.model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name}: {e}\")\n",
    "            dataset_results[model_name] = None\n",
    "    \n",
    "    all_results[dataset_name] = dataset_results\n",
    "    trained_models[dataset_name] = dataset_models\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0947a",
   "metadata": {},
   "source": [
    "## 6. Learning Curves Visualization\n",
    "\n",
    "Let's visualize the learning curves for all models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7efa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Learning Curves: Training Progress Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "model_names = ['Griffin', 'Hawk', 'Local Attention']\n",
    "\n",
    "for dataset_idx, (dataset_name, results) in enumerate(all_results.items()):\n",
    "    # Training loss\n",
    "    ax_train = axes[dataset_idx, 0]\n",
    "    ax_train.set_title(f'{dataset_name}: Training Loss')\n",
    "    \n",
    "    # Validation loss\n",
    "    ax_val = axes[dataset_idx, 1]\n",
    "    ax_val.set_title(f'{dataset_name}: Validation Loss')\n",
    "    \n",
    "    for model_idx, (model_name, history) in enumerate(results.items()):\n",
    "        if history is not None:\n",
    "            color = colors[model_idx % len(colors)]\n",
    "            \n",
    "            # Plot training loss\n",
    "            ax_train.plot(history['epochs'], history['train_loss'], \n",
    "                         color=color, label=model_name, marker='o', linewidth=2)\n",
    "            \n",
    "            # Plot validation loss\n",
    "            ax_val.plot(history['epochs'], history['val_loss'], \n",
    "                       color=color, label=model_name, marker='s', linewidth=2)\n",
    "    \n",
    "    ax_train.set_xlabel('Epoch')\n",
    "    ax_train.set_ylabel('Loss')\n",
    "    ax_train.legend()\n",
    "    ax_train.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax_val.set_xlabel('Epoch')\n",
    "    ax_val.set_ylabel('Loss')\n",
    "    ax_val.legend()\n",
    "    ax_val.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d1b6d",
   "metadata": {},
   "source": [
    "## 7. Perplexity Comparison\n",
    "\n",
    "Let's also look at perplexity curves to understand model performance better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot perplexity curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Validation Perplexity: Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "for dataset_idx, (dataset_name, results) in enumerate(all_results.items()):\n",
    "    ax = axes[dataset_idx]\n",
    "    ax.set_title(f'{dataset_name}: Validation Perplexity')\n",
    "    \n",
    "    for model_idx, (model_name, history) in enumerate(results.items()):\n",
    "        if history is not None:\n",
    "            color = colors[model_idx % len(colors)]\n",
    "            ax.plot(history['epochs'], history['val_perplexity'], \n",
    "                   color=color, label=model_name, marker='o', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Perplexity')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')  # Log scale for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe79fc",
   "metadata": {},
   "source": [
    "## 8. Final Performance Summary\n",
    "\n",
    "Let's create a comprehensive summary of the final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e12202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "summary_data = []\n",
    "\n",
    "for dataset_name, results in all_results.items():\n",
    "    for model_name, history in results.items():\n",
    "        if history is not None and len(history['val_loss']) > 0:\n",
    "            final_train_loss = history['train_loss'][-1]\n",
    "            final_val_loss = history['val_loss'][-1]\n",
    "            final_val_ppl = history['val_perplexity'][-1]\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'Final Train Loss': final_train_loss,\n",
    "                'Final Val Loss': final_val_loss,\n",
    "                'Final Val Perplexity': final_val_ppl,\n",
    "                'Convergence': 'Good' if final_val_loss < final_train_loss * 1.2 else 'Overfitting'\n",
    "            })\n",
    "\n",
    "if summary_data:\n",
    "    import pandas as pd\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    # Create summary visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Final validation loss comparison\n",
    "    sns.barplot(data=df_summary, x='Dataset', y='Final Val Loss', hue='Model', ax=axes[0])\n",
    "    axes[0].set_title('Final Validation Loss Comparison')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Final perplexity comparison\n",
    "    sns.barplot(data=df_summary, x='Dataset', y='Final Val Perplexity', hue='Model', ax=axes[1])\n",
    "    axes[1].set_title('Final Validation Perplexity Comparison')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best performers\n",
    "    print(\"\\nBEST PERFORMERS:\")\n",
    "    print(\"=\" * 30)\n",
    "    for dataset in df_summary['Dataset'].unique():\n",
    "        dataset_df = df_summary[df_summary['Dataset'] == dataset]\n",
    "        best_model = dataset_df.loc[dataset_df['Final Val Loss'].idxmin(), 'Model']\n",
    "        best_loss = dataset_df['Final Val Loss'].min()\n",
    "        print(f\"{dataset}: {best_model} (Loss: {best_loss:.4f})\")\n",
    "else:\n",
    "    print(\"No training results available for summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7239d",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation on Test Set\n",
    "\n",
    "Now let's evaluate our trained models on the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set evaluation\n",
    "print(\"EVALUATING ON TEST SETS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "evaluator = ModelEvaluator(device=device)\n",
    "test_results = []\n",
    "\n",
    "for dataset_name, dataset_info in datasets.items():\n",
    "    print(f\"\\nEvaluating on {dataset_name} test set...\")\n",
    "    \n",
    "    if dataset_name in trained_models:\n",
    "        for model_name, model in trained_models[dataset_name].items():\n",
    "            try:\n",
    "                # Evaluate model\n",
    "                metrics = evaluator.evaluate_model(\n",
    "                    model=model,\n",
    "                    dataloader=dataset_info['test'],\n",
    "                    model_name=model_name,\n",
    "                    dataset_name=dataset_name\n",
    "                )\n",
    "                \n",
    "                test_results.append({\n",
    "                    'Dataset': dataset_name,\n",
    "                    'Model': model_name,\n",
    "                    'Test Loss': metrics.loss,\n",
    "                    'Test Perplexity': metrics.perplexity,\n",
    "                    'Inference Time': metrics.inference_time,\n",
    "                    'Memory Usage': metrics.memory_usage\n",
    "                })\n",
    "                \n",
    "                print(f\"  {model_name}: Loss={metrics.loss:.4f}, PPL={metrics.perplexity:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error evaluating {model_name}: {e}\")\n",
    "\n",
    "if test_results:\n",
    "    df_test = pd.DataFrame(test_results)\n",
    "    \n",
    "    print(\"\\nTEST SET RESULTS:\")\n",
    "    print(df_test.to_string(index=False))\n",
    "    \n",
    "    # Test results visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.barplot(data=df_test, x='Dataset', y='Test Loss', hue='Model', ax=ax)\n",
    "    ax.set_title('Test Set Performance Comparison')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No test results available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a5904",
   "metadata": {},
   "source": [
    "## 10. Save Results\n",
    "\n",
    "Let's save our training results for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5023f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_dir = project_root / 'notebooks' / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save training history\n",
    "results_file = results_dir / f'training_results_{timestamp}.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"Training results saved to: {results_file}\")\n",
    "\n",
    "# Save summary tables\n",
    "if summary_data:\n",
    "    summary_file = results_dir / f'performance_summary_{timestamp}.csv'\n",
    "    df_summary.to_csv(summary_file, index=False)\n",
    "    print(f\"Performance summary saved to: {summary_file}\")\n",
    "\n",
    "if test_results:\n",
    "    test_file = results_dir / f'test_results_{timestamp}.csv'\n",
    "    df_test.to_csv(test_file, index=False)\n",
    "    print(f\"Test results saved to: {test_file}\")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b2b642",
   "metadata": {},
   "source": [
    "## 11. Conclusions\n",
    "\n",
    "This notebook has demonstrated:\n",
    "\n",
    "### Training Process:\n",
    "1. **Setup**: Configured models and datasets for comparative training\n",
    "2. **Training**: Trained all three architectures on both MQAR and Chomsky datasets\n",
    "3. **Monitoring**: Tracked training and validation metrics across epochs\n",
    "4. **Evaluation**: Assessed final performance on test sets\n",
    "\n",
    "### Key Observations:\n",
    "- **Convergence Speed**: How quickly each model learns the tasks\n",
    "- **Generalization**: Performance gap between training and validation\n",
    "- **Task Specificity**: Which architectures excel on which types of problems\n",
    "- **Efficiency**: Training time and memory requirements\n",
    "\n",
    "### Next Steps:\n",
    "1. **Longer Training**: Train for more epochs to see full convergence\n",
    "2. **Hyperparameter Tuning**: Optimize learning rates and architectures\n",
    "3. **More Datasets**: Test on additional sequence modeling tasks\n",
    "4. **Analysis**: Deep dive into what each model learns differently\n",
    "\n",
    "This comparative study provides valuable insights into the trade-offs between different sequence modeling approaches and demonstrates Griffin's hybrid advantage in practice."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
