# Griffin model configuration
vocab_size: 32000
d_model: 512
num_layers: 6
num_heads: 8
max_seq_len: 2048
local_window: 256
gate_type: "glu"
activation: "swish"
mixing_alpha: 0.5
dropout: 0.1
layer_norm: true
tie_embeddings: true

# Training hyperparameters
batch_size: 32
learning_rate: 0.001
weight_decay: 0.01
num_epochs: 20
optimizer: "adamw"
scheduler: "cosine"

# Dataset options
dataset: "MQAR"  # or "Chomsky"
sequence_length: 512
dataset_difficulty: "medium"  # e.g., easy, medium, hard

# Logging and evaluation
logging:
  tensorboard: true
  log_interval: 50
  save_checkpoints: true
  checkpoint_interval: 5
