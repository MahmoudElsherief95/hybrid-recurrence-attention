# Griffin model configuration
vocab_size: 32000
d_model: 512
num_layers: 6
num_heads: 8
max_seq_len: 2048
local_window: 256
gate_type: "glu"
activation: "swish"
mixing_alpha: 0.5
dropout: 0.1
layer_norm: true
tie_embeddings: true
